---
title: 'How Rolldown Works: High-Performance Code Splitting with Bitset Logic'
publishedAt: 2025-12-09
draft: false
tags:
  ['rolldown', 'vite', 'rust', 'chunks']
---

# Introduction

Code generate stage. TODO.

# Generate chunks

Some basic data structure should be introduced before the real content. But we can fold the details, to makes the reader could search them when it's necessary.

```rust
let mut chunk_graph = ChunkGraph::new(self.link_output.module_table.modules.len());
let mut index_splitting_info: IndexSplittingInfo = // BitSet for each module
let mut bits_to_chunk = FxHashMap::with_capacity(self.link_output.entries.len());
let input_base = ArcStr::from(self.get_common_dir_of_all_modules(...));
```


Based on the `self.option.reverse_module`, use different way to handle it.
    - True, generate chunks for each modules
    - False, use algorithm to split the module. We will focus on this whole algorithm

TODO. explain the reverse module logic here, and use following section for the non-reverse module. Basically it includes `init_entry_point` and `split_chunks` functions.

Two main branching strategies.

Path A. preserve modules mode.
Path B. normal bundling mode.

# Normal bundling mode

## Entry point creation

Entry point creation. `init_entry_point()`.

## Split chunks

This function essentially transforms the module dependency graph into an efficient chunk allocation strategy. Also apply automatic splitting logic and user-defined code splitting rules.

The split_chunks function includes several steps:
1. entry analysis. `determine_reachable_modules_for_entry` does BFS on the module graph. Starts from each entry, go through all modules. and assign the entry_index to the modules' bits so that we can split the modules to chunks based on this.
2. manual splitting. according to the user-defined groups, which includes pattern matching and function, to split the modules to chunks first. this is because the user has more business information so that make it better than the auto algo.
3. module assignment. group by bts pattern matching. This is a auto algo to makes the chunks not to large, not too small.
4. if its possible, merge and optimize the chunks.


### Phase 1: Defining Reachability (The BitSet Fingerprint)

When Rolldown performs code splitting, it needs to figure out which modules each entry point can reach. The `determine_reachable_modules_for_entry` function solves this using a breadth-first graph traversal.

The dependencies graph is built during the scan & link stage, the function fetch the dependencies vector via the [module index](abbr:ModuleIdx) on the `link_output.module_table`. The BFS is processed on the module dependency graph via a `VecDeque`, processing modules level by level from the entry point.

For each modules that was scanned, the `entry_index` will be set on its `bits`. This is used for code splitting algorithm, we will discuss it later. All of these information will be stored in the `index_splitting_info`, which is a `IndexVec`. The type defination is `IndexVec<ModuleIdx, SplittingInfo>`. We can use `module_idx` to refer it easily. And the defination of `SplittingInfo` is:

```rust
pub struct SplittingInfo {
  pub bits: BitSet,
  pub share_count: u32,
}
```

This reachability information later determines which modules go into which chunks during the final bundle generation. We will go through the determining process later.

### Phase 2: User-Driven Partitioning (Manual Splitting)

1. Early exit checks. This is used for extract the code splitting related configrations. Exit the function if there's no configrations.

2. Module group creation phase. 

```rust
let mut name_to_module_groups: FxHashMap<(usize, ArcStr), ModuleGroupIdx>
let mut index_module_groups: IndexVec<ModuleGroupIdx, ModuleGroup>
```

This is a two level data structure design. We get the `match_group_index` and [match group](abbr:match_group.md) from the `match_groups` stored in the `chunking_options`, we use it to create a related index. And the group related data is stored underthe `ModuleGroup` data structure, which can be refered by the index, stored in the `IndexVec`. Such design avoid the inefficient linear seach and also good for sorting after this step.

`name_to_module_group` is a hash map that maps `(match_group_index, group_name)` to `ModuleGroupIdx`. And the related data could be found in the `index_module_groups`, which is used for storing module groups.

```rust
struct ModuleGroup {
  name: ArcStr,
  match_group_index: usize,
  modules: FxHashSet<ModuleIdx>,
  priority: u32,
  sizes: f64,
}
```



Next, iterate all modules stored in the module table, which is scanned and processed during the scan & link stage. For each module, use each match group to test it. Determining if current module should be put into this group. User can use regex or function in the match group, Rolldown will execute them to check the current module. After that, the module size will be checked if the `allow_min_module_size` and `allow_max_module_size` is setted. The `match_group_index` and `group_name` will be combined as the unique key of module group. At the end of the processing, if the `include_dependencies_recursively` is true, the dependencies will be added to the group recursively. `add_module_and_dependencies_to_group_recursively` is a classic depth-first recursive function on dependencies graph. Finally, all related modules will be added in the group. Pay attention, all dependencies will be added with bypassing the size and shar-count checks.

In the rolldown's design, the group means root plus deps. And the manual group match is only for the _entry_ selection, nto every dependency.


3. Module group sorting and filtering.

Module group will be sorted according to its priority. For the same priority, the one with lower index goes first. For the same priority and index, use dictionary order to sort. Since the sorted vector will be consumed via `pop`, therefore there is a outer `Reverse`.

4. Runtime Module Handling

Manually pull out the runtime module into a standalone chunk and also remove the runtime module from other module groups if it exists.

5. Main processing loop

This is the core of the manual code splitting algorithm. The module group is a stack, we have already sort it according to the priority in the previous step. Now we can use `pop` to iterate the module groups according to the priority. The module groups which is smaller than the allow min size will be skipped. The large module groups will be splitted and make sure its size in the given scope. And this part is the most interested part.

First, preprocessing the modules in this module group according to its size, stable id and executation order. The purpose of doing this is for better segemation. Because there is a giant module which is bigger than the max group size limitation at left part of the module vector, the algorithm won't be able to segementate it.

Second, algorithm iterate the modules from left and right at the same time. Because the given module group is a range, therefore we need to make sure the division won't create a tiny module group that can't match the minimaize size configuration. More specificaly, if `next_right_index + 1 < next_left_index` is true, it means this group can't be split into two groups with both satisfied the min size requirement. Rolldown will ignore the max size requirement and keep the group as whole in this case.

Once the split index is confirmed, the algorithm will try to let the left group includes as many as possible modules until it reach the allowed max size, while ensuring the right module group matches the requirements. Finally, push these two module group into the module group stack. If the right module group is still can be split, it will follow the same logic in the next loop, otherwise, the module group will be skip until next large module group appears.  

After the module groups split loop finished, the chunks will be created based on the result. And the modules in current module group will be removed from other groups.

### Phase 3: Automated Allocation (The Core Loop)

After the manual module assignment, we will process the rest unprocessed modules. As we mentioned before, all reachable module has been marked the entry in the bit during the graph traversal, and this is the key of chunk generation algorithm. The bits will decides how to place current module. Before delve into the Rolldown's solution, let's consider the basic mental module of this algorithm. 

Let's set the details of this code aside and rethink the wording of this question. In a big project, modules are existing in the module graph. The code execute starting from the entries. If some modules coule be reached from the same entries, it would be better to put them in the same bundle. This could avoid duplicating code across entry chunks, minimize unnecessary sharing. Which is the best strategy for automatic splitting.

Now, the problem has been transform to: give you multiple modules, all of them has been marked the entry. Or in other words, the entry combinations. How to find and classification them according to the entry combinations?

Here is a good [LeetCode problem](https://leetcode.com/problems/find-the-prefix-common-array-of-two-arrays/) which illustrate the basic idea of the solution. Basically, the question is, give you two series of numbers, you need to iterate the given numebrs and calculate the number of prefix common array. Suppose these numbers are the unique index of entry, and every time we process a module during the module graph traveral, we will record a entry unique index on the module, in this problem, it's the number of the number series. Because the entry is not ordered, therefore we need to handle the scenario like `[1, 2]` and `[2, 1]`. A smart solution is using bit. Suppose the number is the position of the bit. Everytime we have a number, set the position x to be 1 on the bit. When we want to compare and check if there is any common prefix array, we can use `&` directly. Here is the solution of the LeetCode problem.

```Python
def findThePrefixCommonArray(self, A: List[int], B: List[int]) -> List[int]:
  a, b = 0, 0
  ans = []

  for x, y in zip(A, B):
      a |= 1 << x
      b |= 1 << y
      ans.append((a & b).bit_count())
    
  return ans
```

Based on the understanding of this usage of bit. Let's back to the Rolldown's automatic code splitting algorithm. During the module traversal stage, all reachable modules are marked the entry unique index in the bit. And all modules that have same bit could be put into the same chunk. In the source code, the bit will be extract from the `index_splitting_info`.

Case A. Chunk for these bit already exists. This is the simplest scenario, just add the module to chunk.

Case B. Keep certain user-defined entry modules in their entry chunk. User-defined entry modules that are not wrapped should stay in their own entry chunk, even when reachable from multiple entires. This avoids creating unnecessary common chunks that would turn the entry into a [facade](abbr:facade_entry.md). Otherwise, the browser would waste more time to load the code for first screen rendering.

Case C. Chunk optimization path. The modules would be insert into the `pending_commpon_chunks` to later try smarter merging. Bacause the optimization algorithm needs more information about the common modules.

Case D. Default. Create a new common chunk for this bitset, and record the mapping relationship between the bit and chunk.


### Phase 4: Structural Optimization (Merging & Facades)

Since these two optimization algorithm includes more information that we can explain all details in one article, we will only focus on the high level understanding.

First is trying to insert common module to exist chunk. Instead of creating new common chunks for shared modules directly, the optimizer iterates through pending common chunks and try to merge the modules into an existing entry chunk if it's safe to do. The _safe_ means: Dependency / order safe, async boundary safe, dynamic-import safe, and API-shape safe.

Second optimization function is `optimize_facade_dynamic_entry_chunks`. During previous processing, some entry chunk maybe empty as the modules were moved to a common chunk. The function will remove such facade chunk and patches symbol / runtime inclusion so behavior matches as if the facade chunk still existed.


# Post processing phase

After the basic chunks created, some post processing need to be done before writing the real result to the disk. We will focus on the high level understanding.

## Merge external import namespaces at chunk level

During the scan & link stage, the [external namespace](abbr:ExternalNamespace) was collected and putted into the merger. Since the symbol linking can only happens in the internal of chunks, now it's time to finish that work.

Basically, Rolldown will re-grouping the symbols based on the chunks they belongs to, since the symbol linking could only happen in the same chunk. 

Rolldown uses the logic in your code to perform Symbol Deduplication for these namespaces. Specifically, it handles the scenario where multiple different modules within your project all import the same external library as a namespace.

Before Merging:

```javascript
// Module A (in Chunk 1)
import * as React_1 from 'react';

// Module B (in Chunk 1)
import * as React_2 from 'react';

// Even though they are the same library, they start as separate symbols
```

After Merging:

```javascript
// Rolldown keeps only one namespace reference
import * as React_Namespace from 'react';

// All code that originally used React_1 or React_2 
// is rewritten to use React_Namespace
console.log(React_Namespace.useState);
```

## Sort modules & chunks

All modules in every chunks will be sorted by execution order. After that, the chunks will also be sorted according to the chunks's kind. 

## Find entry-level external modules

The entry-level external modules is an __external module__ that is reachable from an __entry module__ by following only `export *` edges through normal modules. Here is an example:

```javascript
// entry.js
export * from './a.js'

// a.js
export * from 'external'
```

Such relationship will affect `re-export-all` surface where the set of exported names is _not_ statically known to the bundler. This function will idenfify external modules re-exported via `export *` from each entry, tag the exact import records as "entry-level external", and then __recompute dynamic-export metadata__ for the following processing.

# Summary

TODO

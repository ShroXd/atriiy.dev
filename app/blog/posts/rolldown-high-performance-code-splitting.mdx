---
title: 'How Rolldown Works: High-Performance Code Splitting with Bitset Logic'
publishedAt: 2026-01-28
draft: false
tags:
  ['rolldown', 'vite', 'rust', 'chunks']
---

# Introduction

Rolldown is a powerful tool for high-performance code bundling, Vite is planning integrate Rolldown as its default bundler.

The bundling process includes three major stages: module scanning, symbol linking and final code generation. I have write two articles to explain the detail of previous two stages, you can check them here:

- [How Rolldown Works: Module Loading, Dependency Graphs, and Optimization Explained](https://www.atriiy.dev/blog/rolldown-module-loader-and-dependency-graph)
- [How Rolldown Works: Symbol Linking, CJS/ESM Resolution, and Export Analysis Explained](https://www.atriiy.dev/blog/rolldown-link-stage-symbol-linking-resolution)

After previous two stages, we already have precise information about the whole project. From high level module graph to underlying symbol relationships. Therefore, in the last stage, we will use these information to generate the bundling result.

There're two major strategy of code generation. Preserve mode and normal mode. In the preserve mode, Rolldown will create seperate chunks for each modules using the original modules name as the file name. You can check more details [here](https://rolldown.rs/reference/OutputOptions.preserveModules#preservemodules). The source code about this part is pretty straightforward, thus we will focus on the normal mode.

In the normal mode, the module graph from previous stage will be transformed to a series of chunks. The most interesting thing of this stage is how the code splitting algorithm implemented to generate suitable chunks with high performance.

Finally, we will introduce the post processing phase. This phase includes some optimizations, but we will only focus on the high level explanation instead of the code implementation.

# Generate chunks

This is the function for chunk generation of the generate stage. Because the whole generation stage includes a bunch of steps to handle the namespaces, chunk linking, wrapper, etc. This article will only focus on the chunk generation processing and algorithm. And it's what this function does. 

The basic structure of this function is pretty simple. First, it will create some basic data structures. You don't need to worry about the details of these data structures.

```rust
let mut chunk_graph = ChunkGraph::new(self.link_output.module_table.modules.len());
let mut index_splitting_info: IndexSplittingInfo = // BitSet for each module
let mut bits_to_chunk = FxHashMap::with_capacity(self.link_output.entries.len());
let input_base = ArcStr::from(self.get_common_dir_of_all_modules(...));
```

Next, according to the mode configuration, the function uses different strategies for the processing. In the preverse mode, it traverse the module table binded on the  link_output. The chunk will be created directly and added to the chunk graph.

But for the normal bundling mode, the function will use a different approach.

# Normal bundling mode

This is the normally used mode in the general JavaScript project. In this mode, Rolldown transforms the module graph to a chunk graph. It creates entry points according to the [entries](abbr:entries.md). And then traverse all reachable modules from these entries. After that, Rolldown performs manual splitting first to split the high priority modules to chunks. The manual splitting is configured by the user. Then the remaining modules will be processed via the automatic splitting. Finally, Rolldown performs some structural optimization to reduce the unnecessary duplicated modules between different chunks and remove uncessary facades.

## Entry point creation

Since the module traversal is starting from the entry, it's important to init the entry point at first. In the bundling, entry is just a special module. Therefore this function iterates the `entry_point` on the link output and create chunk for each of them.

The most import information of the entry chunk is the `bits`. We will explain the algorithm based on it later, but basically, it's just a wrapped data structure, used like the binary number. The bit length is the length of entries. The normal module will also use the same logic to create the bit of them so that we can use the bit position to record which entry can reach current module. This is important for the code splitting algorithm.

## Split chunks

This function essentially transforms the module dependency graph into an efficient chunk allocation strategy. Also apply automatic splitting logic and user-defined code splitting rules.

The split_chunks function includes several steps:
1. entry analysis. `determine_reachable_modules_for_entry` does BFS on the module graph. Starts from each entry, go through all modules. and assign the entry_index to the modules' bits so that we can split the modules to chunks based on this.
2. manual splitting. according to the user-defined groups, which includes pattern matching and function, to split the modules to chunks first. this is because the user has more business information so that make it better than the auto algo.
3. module assignment. group by bts pattern matching. This is a auto algo to makes the chunks not to large, not too small.
4. if its possible, merge and optimize the chunks.

User intent is high-priority; we carve out what the user specifically asked for first, then let the algorithm solve the remaining 'puzzle' of shared dependencies.


### Phase 1: Defining Reachability (The BitSet Fingerprint)

The first step of code splitting algorithm is to analyze all reachable modules, determine which modules could be reached from which entry. An _entry_ is a root module that Rolldown treats as a chunk starting point. It includes [three different kinds](abbr:entry_types.md) in Rolldown. Start from these entries, Rolldown could find all reachable modules, and mark the starting entry on the module. The purpose of doing this is to make sure the module still keeps reachable from the entry point after code splitting. Because after the code splitting, the source code would be split into multiple chunks, and the chunk is the resource that browser will fetch for rendering. Each chunk is a separate file that can be loaded asynchronously. Therefore it's important to ensure the topological relationships of modules won't be broken after code splitting.

The function utilizes a classic breadth-first graph traversal algorithm on the module graph. Function starts from the entry points, for all new normal module, and also not be marked as not included, it will write the bit and share counting data. Rolldown uses an array, whose type is `SplittingInfo`, to mapping the module index to the splitting information.

```rust
// Type definition for SplittingInfo
pub struct SplittingInfo {
  pub bits: BitSet,
  pub share_count: u32,
}

index_splitting_info[module_idx].bits.set_bit(entry_index);
index_splitting_info[module_idx].share_count += 1;
```

As we can see, the splitting information includes a field `bits` which represents the bit mask for each entry point. The `bits` field is originally set to `0`, but the length is the length of the `entry_points` vector. Assign the bit mask for each entry point, like the following table. If a module is reachable from Entry A, the first bit position will be set to `1` on that module.

Suppose we have three entry points: Entry A, Entry B, and Entry C. The bit mask for each entry point is as follows:

| Entry | Bit |
|-------|-----|
| Entry A | 0 |
| Entry B | 1 |
| Entry C | 2 |

The algorithm would be as follows:

<img
  className='!my-4 block max-w-md'
  alt='Bit Mask for Entry Points'
  src='https://raw.githubusercontent.com/ShroXd/img-hosting/refs/heads/main/blog/2026-02-04-15-37-05.png'
/>

This reachability information later determines which modules go into which chunks during the final bundle generation. We will go through the determining process later.

### Phase 2: User-Driven Partitioning (Manual Splitting)

Based on the reachability information, Rolldown runs first round code splitting. The splitting rules is defined by user's configuration, the [match group](abbr:match_group.md). These rules have higher priority than the automatic splitting, thus Rolldown does this first.

Here is the core data structure of this manual splitting algorithm:

```rust
let mut name_to_module_groups: FxHashMap<(usize, ArcStr), ModuleGroupIdx>
let mut index_module_groups: IndexVec<ModuleGroupIdx, ModuleGroup>
```

The `name_to_module_groups` makes the mapping from user defined group name to module group index easily. And the module group index can be used to access the related module group. Here is the defination of module group, you can understand it as a bucket which will hold the modules assigned to it.

```rust
struct ModuleGroup {
  name: ArcStr,
  match_group_index: usize,
  // Modules belonging to this group
  modules: FxHashSet<ModuleIdx>, 
  priority: u32,
  sizes: f64,
}
```

Next, modules will be added to corresponding module group accroding to the result of rule test. Iterate all modules and try each match group to check if current module is matched. Not matched modules will be skipped. The matched modules' size will be checked if the `allow_min_module_size`, `allow_max_module_size`, and `allow_min_share_count` is setted. If all checks passed, the module will be added to the corresponding group. The group can be accessed via the unique key. Rolldown utilizes the combination of `match_group_index` and `group_name` as the unique key, which is defined in the `name_to_module_groups`'s type. Finally, the module's dependencies will be added to the group recursively via the function `add_module_and_dependencies_to_group_recursively`.

Then, module groups will be sorted according to its priority. For the same priority, the one with lower index goes first. For the same priority and index, use dictionary order to sort. The sorted result is reversed for easier consumption later. Also, the runtime module will also be added into a standalone chunk.

After previous steps, we have bunch of module groups. These module groups need to be splitted or tweaks to make sure the size of each group is in the given scope, so that we can generate the final chunks based on them. Basically, Rolldown iterates the module groups and for each group, tries to cut off segment one by one from the left of the module group vector if that group is larger than the given scope. The splitting algorithm includes the following steps:

1. Preprocessing, sort the modules in the current module group according to its size, stable id and executation order. Smaller modules will be put at the left side of the vector, which is easier to divide a segment whose size is in the given scope. Without this sorting, in some edge cases, the gaint module at left could prevent the splitting.

2. Try to split greedily. Algorithm iterate the modules starts from left and right, try to find two segments whose size is larger than `allow_min_size`. The segment at left would be the result segment, and the right segment is used for checking if the splitting is valid. Because if two segments is overlapped, current group can't be split into two groups with both satisfied the min size requirement. In that case, Rolldown will ignore the max size requirement and keep the group as whole in this case. This makes sure that the browser won't send many http request for loading small files. 

3. Add more modules to splitted group as many as possible. As long as the size of group is lower than the max size reuqirement, it's valid. This makes the final chunks' number small, which is good for the browser loading performance.

4. Add splitted group and remaining right part to the module group stack. If the remaining group is still larger than max size requirement, it will be splitted in the next loop with the same logic.

After the module groups split loop finished, the chunks will be created based on the result. And the modules in current module group will be removed from other groups.

### Phase 3: Automated Allocation (The Core Loop)

Manual module assignment processed the high priority modules, then Rolldown executed the automatic allocation to the remaining modules. The automatic allocation algorithm will assign modules to corresponding chunks based on their __bits__ and also create shared chunks to store modules that belongs to multiple chunks.

__Warm up__

Before explaining the algorithm details, let's consider an interesting [LeetCode problem](https://leetcode.com/problems/find-the-prefix-common-array-of-two-arrays/). Here is the problem description:

_You are given two 0-indexed integer permutations A and B of length n. A prefix common array of A and B is an array C such that `C[i]` is equal to the count of numbers that are present at or before the index `i` in both A and B. Return the __prefix common array__ of A and B. A sequence of n integers is called a permutation if it contains all integers from 1 to n exactly once._

In the example 1, the input is A = [1,3,2,5], B = [3,1,2,4], and the output would be [0,2,3,0]. Although the input is two arrays, we can treat them as a series of pairs since the problem needs use to iterate over them. Therefore we actually trying to compare two arrays in a pair, see if they have same elements.

- At i = 0, the pair is [1] and [3].
- At i = 1, the pair is [1,3] and [3,1].
- At i = 2, the pair is [1,3,2] and [3,1,2].
- At i = 4, the pair is [1,3,2,5] and [3,1,2,4].

Actually, for each `i`, the problems are equal. Give you of two arrays with same length, check if they have same elements. A brute force solution would be to use a hash map or nested loops, but it's not efficient. Bits manipulation is a better choice.

We can use a bit mask to represent the presence of each number. For example, if the array is [1, 3], the bit mask would be `0b01010`. And for another array [1, 2], the bit mask would be `0b00110`. Then calculate the result of `&` between two bit masks.

<img
  className='!my-4 block max-w-md'
  alt='Bit calculation'
  src='https://raw.githubusercontent.com/ShroXd/img-hosting/refs/heads/main/blog/2026-02-05-10-57-27.png'
/>

As the illustration shows, the result is `0b000010`. The number of `1` in the binary representation of the result is the number of common elements. This is easily to get via built-in method. Although the time complexity is O(N), such bit manipulation in many programming languages could use the hardware support to speed up the calculation, which is extremely fast in real project.

Based on the discussion above, the solution of this problem would be:

```python3
def findThePrefixCommonArray(self, A: List[int], B: List[int]) -> List[int]:
  a, b = 0, 0
  ans = []

  for x, y in zip(A, B):
      a |= 1 << x
      b |= 1 << y
      ans.append((a & b).bit_count())
    
  return ans
```

__Back to Rolldown automatic allocation__

I believe you have already understand the idea of the LeetCode problem, but what is the relationship with our Rolldown source code? The solution actually indicate a awsome idea, that we can use bit mask to represent a series of element, so that find the common element or _check if two groups are equal_ from two __unsorted__ arrays based on the bit calculation.

If we can check the equality of unsorted arrays based on the bits, it's also means the __bits__ could be understand the unique key of each array, which makes us easily to calculate the number of __different__ (when I say different, I mean the arrays have different elements, instead of same elements with different order) arrays.

In phase 1, each entry have their unique index and been setted on the module via the method `set_bit`. The bits is actually the unique key of each module group, just like the bits of the given arrays pair in the LeetCode problem. Therefore we can use a _map_ to allocate the module groups, the module groups who have same `bits` could be in the same chunk. The source code of this part is:

```rust
let bits = &index_splitting_info[normal_module.idx].bits;
if let Some(chunk_id) = bits_to_chunk.get(bits).copied() {
    // Add module to chunk
}
// ...
```

Pretty straight forward, right?

Of course, the real source code needs to handle more scenarios. And the code above is the case _A_. In the case _B_, for the user defined entry, Rolldown will try to keep them in their own entry chunk. This avoids creating unnecessary common chunks that would turn the entry into a tiny 1 line [facade](abbr:facade_entry.md) file.

Suppose we configure two entries:

```typescript
// src/admin.ts
export const runAdmin = () => console.log('Admin');

// src/main.ts
import { runAdmin } from './admin.ts';
runAdmin();
```

The `admin.ts` is a user-defined entry, but it is also reachable from `main.ts` via a normal static import. Without the optimization in case B, Rolldown notices that the `admin.ts` is referenced by multiple entry points, which is good to store it in the shared chunk. So `admin.ts` moves into something like `chunk-XYZ.js`, and the `admin` entry becomes a __facade entry__ whose job is just _re-export everything from the real chunk`.

```javascript
// chunk-xyz
export function runAdmin() { console.log('Admin'); }

// chunk-admin
export * from './chunk-xyz.js';

// chunk-main
import { runAdmin } from './chunk-xyz.js';
runAdmin();
```

Such tiny facade chunks would increase the number of HTTP requests, which is bad for HTTP/2 multiplexing and initial load.

In the case _C_, The modules would be insert into the `pending_commpon_chunks` to later try smarter merging. Bacause the optimization algorithm needs more information about the common modules.

And the final case D is the default case. If the target bits is not exist, create a new common chunk for this bitset, and record the mapping relationship between the bit and chunk.

### Phase 4: Structural Optimization (Merging & Facades)

Since these two optimization algorithm includes more information than we can explain all details in one article, we will only focus on the high level understanding.

First optimization is trying to insert common module to exist chunk. Instead of creating new common chunks for shared modules directly, the optimizer iterates through pending common chunks and try to merge the modules into an existing entry chunk if it's safe to do. The _safe_ means: Dependency / order safe, async boundary safe, dynamic-import safe, and API-shape safe.

Second optimization function is `optimize_facade_dynamic_entry_chunks`. During previous processing, some entry chunk maybe empty as the modules were moved to a common chunk. The function will remove such facade chunk and patches symbol / runtime inclusion so behavior matches as if the facade chunk still existed. The purpose is similar to the user defined facade chunk opitmization, but it works for the empty entry chunks.

# Post processing phase

Once the physical boundaries of the chunks are decided, Rolldown must perform a final 'polish' to ensure symbols are correctly wired and the output is optimized for execution

## Merge external import namespaces at chunk level

During the scan & link stage, the [external namespace](abbr:ExternalNamespace) was collected and putted into the merger. Since the symbol linking can only happens in the internal of chunks, now it's time to finish that work.

Basically, Rolldown will re-grouping the symbols based on the chunks they belongs to, since the symbol linking could only happen in the same chunk. 

Rolldown uses the logic in your code to perform Symbol Deduplication for these namespaces. Specifically, it handles the scenario where multiple different modules within your project all import the same external library as a namespace.

Before Merging:

```javascript
// Module A (in Chunk 1)
import * as React_1 from 'react';

// Module B (in Chunk 1)
import * as React_2 from 'react';

// Even though they are the same library, they start as separate symbols
```

After Merging:

```javascript
// Rolldown keeps only one namespace reference
import * as React_Namespace from 'react';

// All code that originally used React_1 or React_2 
// is rewritten to use React_Namespace
console.log(React_Namespace.useState);
```

## Sort modules & chunks

All modules in every chunks will be sorted by execution order. After that, the chunks will also be sorted according to the chunks's kind. 

## Find entry-level external modules

The entry-level external modules is an __external module__ that is reachable from an __entry module__ by following only `export *` edges through normal modules. Here is an example:

```javascript
// entry.js
export * from './a.js'

// a.js
export * from 'external'
```

Such relationship will affect `re-export-all` surface where the set of exported names is _not_ statically known to the bundler. This function will idenfify external modules re-exported via `export *` from each entry, tag the exact import records as "entry-level external", and then __recompute dynamic-export metadata__ for the following processing.

# Summary

TODO

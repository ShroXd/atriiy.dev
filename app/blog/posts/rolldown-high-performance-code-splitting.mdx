---
title: 'How Rolldown Works: High-Performance Code Splitting with Bitset Logic'
publishedAt: 2026-01-28
draft: false
tags:
  ['rolldown', 'vite', 'rust', 'chunks']
---

# Introduction

Rolldown is a powerful tool for high-performance code bundling, Vite is planning integrate Rolldown as its default bundler.

The bundling process includes three major stages: module scanning, symbol linking and final code generation. I have write two articles to explain the detail of previous two stages, you can check them here:

- [How Rolldown Works: Module Loading, Dependency Graphs, and Optimization Explained](https://www.atriiy.dev/blog/rolldown-module-loader-and-dependency-graph)
- [How Rolldown Works: Symbol Linking, CJS/ESM Resolution, and Export Analysis Explained](https://www.atriiy.dev/blog/rolldown-link-stage-symbol-linking-resolution)

After previous two stages, we already have precise information about the whole project. From high level module graph to underlying symbol relationships. Therefore, in the last stage, we will use these information to generate the bundling result.

There're two major strategy of code generation. Preserve mode and normal mode. In the preserve mode, Rolldown will create seperate chunks for each modules using the original modules name as the file name. You can check more details [here](https://rolldown.rs/reference/OutputOptions.preserveModules#preservemodules). The source code about this part is pretty straightforward, thus we will focus on the normal mode.

In the normal mode, the module graph from previous stage will be transformed to a series of chunks. The most interesting thing of this stage is how the code splitting algorithm implemented to generate suitable chunks with high performance.

Finally, we will introduce the post processing phase. This phase includes some optimizations, but we will only focus on the high level explanation instead of the code implementation.

# Generate chunks

This is the function for chunk generation of the generate stage. Because the whole generation stage includes a bunch of steps to handle the namespaces, chunk linking, wrapper, etc. This article will only focus on the chunk generation processing and algorithm. And it's what this function does. 

The basic structure of this function is pretty simple. First, it will create some basic data structures. You don't need to worry about the details of these data structures.

```rust
let mut chunk_graph = ChunkGraph::new(self.link_output.module_table.modules.len());
let mut index_splitting_info: IndexSplittingInfo = // BitSet for each module
let mut bits_to_chunk = FxHashMap::with_capacity(self.link_output.entries.len());
let input_base = ArcStr::from(self.get_common_dir_of_all_modules(...));
```

Next, according to the mode configuration, the function uses different strategies for the processing. In the preverse mode, it traverse the module table binded on the  link_output. The chunk will be created directly and added to the chunk graph.

But for the normal bundling mode, the function will use a different approach.

# Normal bundling mode

This is the normally used mode in the general JavaScript project. In this mode, Rolldown transforms the module graph to a chunk graph. It creates entry points according to the [entries](abbr:entries.md). And then traverse all reachable modules from these entries. After that, Rolldown performs manual splitting first to split the high priority modules to chunks. The manual splitting is configured by the user. Then the remaining modules will be processed via the automatic splitting. Finally, Rolldown performs some structural optimization to reduce the unnecessary duplicated modules between different chunks and remove uncessary facades.

## Entry point creation

Since the module traversal is starting from the entry, it's important to init the entry point at first. In the bundling, entry is just a special module. Therefore this function iterates the `entry_point` on the link output and create chunk for each of them.

The most import information of the entry chunk is the `bits`. We will explain the algorithm based on it later, but basically, it's just a wrapped data structure, used like the binary number. The bit length is the length of entries. The normal module will also use the same logic to create the bit of them so that we can use the bit position to record which entry can reach current module. This is important for the code splitting algorithm.

## Split chunks

This function essentially transforms the module dependency graph into an efficient chunk allocation strategy. Also apply automatic splitting logic and user-defined code splitting rules.

The split_chunks function includes several steps:
1. entry analysis. `determine_reachable_modules_for_entry` does BFS on the module graph. Starts from each entry, go through all modules. and assign the entry_index to the modules' bits so that we can split the modules to chunks based on this.
2. manual splitting. according to the user-defined groups, which includes pattern matching and function, to split the modules to chunks first. this is because the user has more business information so that make it better than the auto algo.
3. module assignment. group by bts pattern matching. This is a auto algo to makes the chunks not to large, not too small.
4. if its possible, merge and optimize the chunks.

User intent is high-priority; we carve out what the user specifically asked for first, then let the algorithm solve the remaining 'puzzle' of shared dependencies.


### Phase 1: Defining Reachability (The BitSet Fingerprint)

The first step of code splitting algorithm is to analyze all reachable modules, determine which modules could be reached from which entry. An _entry_ is a root module that Rolldown treats as a chunk starting point. It includes [three different kinds](abbr:entry_types.md) in Rolldown. Start from these entries, Rolldown could find all reachable modules, and mark the starting entry on the module. The purpose of doing this is to make sure the module still keeps reachable from the entry point after code splitting. Because after the code splitting, the source code would be split into multiple chunks, and the chunk is the resource that browser will fetch for rendering. Each chunk is a separate file that can be loaded asynchronously. Therefore it's important to ensure the topological relationships of modules won't be broken after code splitting.

The function utilizes a classic breadth-first graph traversal algorithm on the module graph. Function starts from the entry points, for all new normal module, and also not be marked as not included, it will write the bit and share counting data. Rolldown uses an array, whose type is `SplittingInfo`, to mapping the module index to the splitting information.

```rust
// Type definition for SplittingInfo
pub struct SplittingInfo {
  pub bits: BitSet,
  pub share_count: u32,
}

index_splitting_info[module_idx].bits.set_bit(entry_index);
index_splitting_info[module_idx].share_count += 1;
```

As we can see, the splitting information includes a field `bits` which represents the bit mask for each entry point. The `bits` field is originally set to `0`, but the length is the length of the `entry_points` vector. Assign the bit mask for each entry point, like the following table. If a module is reachable from Entry A, the first bit position will be set to `1` on that module.

Suppose we have three entry points: Entry A, Entry B, and Entry C. The bit mask for each entry point is as follows:

| Entry | Bit |
|-------|-----|
| Entry A | 0 |
| Entry B | 1 |
| Entry C | 2 |

The algorithm would be as follows:

<img
  className='!my-4 block max-w-md'
  alt='Bit Mask for Entry Points'
  src='https://raw.githubusercontent.com/ShroXd/img-hosting/refs/heads/main/blog/2026-02-04-15-37-05.png'
/>

This reachability information later determines which modules go into which chunks during the final bundle generation. We will go through the determining process later.

### Phase 2: User-Driven Partitioning (Manual Splitting)

Suggestion: Focus on the Constraint Solving aspect. The logic about next_right_index and min_size is essentially a bin-packing problem. Frame it as: "How Rolldown balances user size constraints with the need to split code."

Based on the reachability information, Rolldown runs first round code splitting. The splitting rules is defined by user's configuration, the [match group](abbr:match_group.md). These rules have higher priority than the automatic splitting, thus Rolldown does this first.

Here is the core data structure of this manual splitting algorithm:

```rust
let mut name_to_module_groups: FxHashMap<(usize, ArcStr), ModuleGroupIdx>
let mut index_module_groups: IndexVec<ModuleGroupIdx, ModuleGroup>
```

The `name_to_module_groups` makes the mapping from user defined group name to module group index easily. And the module group index can be used to access the related module group. Here is the defination of module group, you can understand it as a bucket which will hold the modules assigned to it.

```rust
struct ModuleGroup {
  name: ArcStr,
  match_group_index: usize,
  // Modules belonging to this group
  modules: FxHashSet<ModuleIdx>, 
  priority: u32,
  sizes: f64,
}
```

Next, modules will be added to corresponding module group accroding to the result of rule test. Iterate all modules and try each match group to check if current module is matched. Not matched modules will be skipped. The matched modules' size will be checked if the `allow_min_module_size`, `allow_max_module_size`, and `allow_min_share_count` is setted. If all checks passed, the module will be added to the corresponding group. The group can be accessed via the unique key. Rolldown utilizes the combination of `match_group_index` and `group_name` as the unique key, which is defined in the `name_to_module_groups`'s type. Finally, the module's dependencies will be added to the group recursively via the function `add_module_and_dependencies_to_group_recursively`.

Then, module groups will be sorted according to its priority. For the same priority, the one with lower index goes first. For the same priority and index, use dictionary order to sort. The sorted result is reversed for easier consumption later. Also, the runtime module will also be added into a standalone chunk.



5. Main processing loop

This is the core of the manual code splitting algorithm. The module group is a stack, we have already sort it according to the priority in the previous step. Now we can use `pop` to iterate the module groups according to the priority. The module groups which is smaller than the allow min size will be skipped. The large module groups will be splitted and make sure its size in the given scope. And this part is the most interested part.

First, preprocessing the modules in this module group according to its size, stable id and executation order. The purpose of doing this is for better segemation. Because there is a giant module which is bigger than the max group size limitation at left part of the module vector, the algorithm won't be able to segementate it.

Second, algorithm iterate the modules from left and right at the same time. Because the given module group is a range, therefore we need to make sure the division won't create a tiny module group that can't match the minimaize size configuration. More specificaly, if `next_right_index + 1 < next_left_index` is true, it means this group can't be split into two groups with both satisfied the min size requirement. Rolldown will ignore the max size requirement and keep the group as whole in this case.

Once the split index is confirmed, the algorithm will try to let the left group includes as many as possible modules until it reach the allowed max size, while ensuring the right module group matches the requirements. Finally, push these two module group into the module group stack. If the right module group is still can be split, it will follow the same logic in the next loop, otherwise, the module group will be skip until next large module group appears.  

After the module groups split loop finished, the chunks will be created based on the result. And the modules in current module group will be removed from other groups.

### Phase 3: Automated Allocation (The Core Loop)

Logical Tweak: You mention "Case B" (keeping user-defined entries). This is crucial. Maybe explain that if we didn't do this, every entry would become a tiny 1-line "facade" file that just imports from a giant shared chunk, which is bad for HTTP/2 multiplexing and initial load.

After the manual module assignment, we will process the rest unprocessed modules. As we mentioned before, all reachable module has been marked the entry in the bit during the graph traversal, and this is the key of chunk generation algorithm. The bits will decides how to place current module. Before delve into the Rolldown's solution, let's consider the basic mental module of this algorithm. 

Let's set the details of this code aside and rethink the wording of this question. In a big project, modules are existing in the module graph. The code execute starting from the entries. If some modules coule be reached from the same entries, it would be better to put them in the same bundle. This could avoid duplicating code across entry chunks, minimize unnecessary sharing. Which is the best strategy for automatic splitting.

Now, the problem has been transform to: give you multiple modules, all of them has been marked the entry. Or in other words, the entry combinations. How to find and classification them according to the entry combinations?

Here is a good [LeetCode problem](https://leetcode.com/problems/find-the-prefix-common-array-of-two-arrays/) which illustrate the basic idea of the solution. Basically, the question is, give you two series of numbers, you need to iterate the given numebrs and calculate the number of prefix common array. Suppose these numbers are the unique index of entry, and every time we process a module during the module graph traveral, we will record a entry unique index on the module, in this problem, it's the number of the number series. Because the entry is not ordered, therefore we need to handle the scenario like `[1, 2]` and `[2, 1]`. A smart solution is using bit. Suppose the number is the position of the bit. Everytime we have a number, set the position x to be 1 on the bit. When we want to compare and check if there is any common prefix array, we can use `&` directly. Here is the solution of the LeetCode problem.

```Python
def findThePrefixCommonArray(self, A: List[int], B: List[int]) -> List[int]:
  a, b = 0, 0
  ans = []

  for x, y in zip(A, B):
      a |= 1 << x
      b |= 1 << y
      ans.append((a & b).bit_count())
    
  return ans
```

Based on the understanding of this usage of bit. Let's back to the Rolldown's automatic code splitting algorithm. During the module traversal stage, all reachable modules are marked the entry unique index in the bit. And all modules that have same bit could be put into the same chunk. In the source code, the bit will be extract from the `index_splitting_info`.

Case A. Chunk for these bit already exists. This is the simplest scenario, just add the module to chunk.

Case B. Keep certain user-defined entry modules in their entry chunk. User-defined entry modules that are not wrapped should stay in their own entry chunk, even when reachable from multiple entires. This avoids creating unnecessary common chunks that would turn the entry into a [facade](abbr:facade_entry.md). Otherwise, the browser would waste more time to load the code for first screen rendering.

Case C. Chunk optimization path. The modules would be insert into the `pending_commpon_chunks` to later try smarter merging. Bacause the optimization algorithm needs more information about the common modules.

Case D. Default. Create a new common chunk for this bitset, and record the mapping relationship between the bit and chunk.


### Phase 4: Structural Optimization (Merging & Facades)

Suggestion: focusing on the "Empty Facade" problem. Itâ€™s a common frustration in bundlers (getting a index.js that only contains export * from './chunk-xyz.js'). Mentioning that Rolldown actively cleans these up adds a lot of "value" to the reader.

Since these two optimization algorithm includes more information that we can explain all details in one article, we will only focus on the high level understanding.

First is trying to insert common module to exist chunk. Instead of creating new common chunks for shared modules directly, the optimizer iterates through pending common chunks and try to merge the modules into an existing entry chunk if it's safe to do. The _safe_ means: Dependency / order safe, async boundary safe, dynamic-import safe, and API-shape safe.

Second optimization function is `optimize_facade_dynamic_entry_chunks`. During previous processing, some entry chunk maybe empty as the modules were moved to a common chunk. The function will remove such facade chunk and patches symbol / runtime inclusion so behavior matches as if the facade chunk still existed.

# Post processing phase

Once the physical boundaries of the chunks are decided, Rolldown must perform a final 'polish' to ensure symbols are correctly wired and the output is optimized for execution

## Merge external import namespaces at chunk level

During the scan & link stage, the [external namespace](abbr:ExternalNamespace) was collected and putted into the merger. Since the symbol linking can only happens in the internal of chunks, now it's time to finish that work.

Basically, Rolldown will re-grouping the symbols based on the chunks they belongs to, since the symbol linking could only happen in the same chunk. 

Rolldown uses the logic in your code to perform Symbol Deduplication for these namespaces. Specifically, it handles the scenario where multiple different modules within your project all import the same external library as a namespace.

Before Merging:

```javascript
// Module A (in Chunk 1)
import * as React_1 from 'react';

// Module B (in Chunk 1)
import * as React_2 from 'react';

// Even though they are the same library, they start as separate symbols
```

After Merging:

```javascript
// Rolldown keeps only one namespace reference
import * as React_Namespace from 'react';

// All code that originally used React_1 or React_2 
// is rewritten to use React_Namespace
console.log(React_Namespace.useState);
```

## Sort modules & chunks

All modules in every chunks will be sorted by execution order. After that, the chunks will also be sorted according to the chunks's kind. 

## Find entry-level external modules

The entry-level external modules is an __external module__ that is reachable from an __entry module__ by following only `export *` edges through normal modules. Here is an example:

```javascript
// entry.js
export * from './a.js'

// a.js
export * from 'external'
```

Such relationship will affect `re-export-all` surface where the set of exported names is _not_ statically known to the bundler. This function will idenfify external modules re-exported via `export *` from each entry, tag the exact import records as "entry-level external", and then __recompute dynamic-export metadata__ for the following processing.

# Summary

TODO

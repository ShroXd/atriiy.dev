---
title: 'Stream Platinum: GStreamer x Rust - Encoding and Decoding'
publishedAt: 2025-03-07
tags: ['gstreamer', 'rust', 'encoding', 'decoding']
draft: true
description: ''
---

We have introduced the basic knowledge about GStreamer and its rust bindings in the previous article. And we also implemented a simple pipeline which only output the testing view on the screen. In the real world pipeline, we need to utilize the GStreamer to deconde, encode, transform the data stream according to the requirements.

It’s important to understand the foundational concepts of the audio and video like container formats, encoding formats, decoding formats, and raw files. These understanding can help us to choose the appropriated elements and fix the problem thrown by the pipeline.

In this article, we will discuss the foundational knowledge of the audio & video, which helps us to understanding and manipulating the media data. Then, we will introduce the related elements and bins in the GStreamer framework. We will focus on decoding/encoding, muxer/demuxer. These elements give us ability to extract raw media stream from any media file and output the processed result. It’s also interesting to handle the raw media stream, we can do many cool things on it. But consider the article length, we will discuss them in the following articles.

## Audio-video fundamentals

The funcdamentals knowledge is essential for us to understand the media processing. GStreamer is a specific media processing framework, there’re many another excellent framwork like [ffmpeg](https://www.ffmpeg.org/). Mastering the fundamental knowledge allows us to choose appropriate tools and gives us deep insight about the framework design.

We will discuss audio-video encoding in this section. This includes the explaination about the encoding and related formats. Additionally, we will discuss the **container** and **raw stream**. These two concepts is related to the encoding deeply. In a nutshell, container holds encoded audio and video streams. The raw audio and video streams generated by recording devices can be encoded, we can also decode the audio and video streams extracted from container file for processing or editing.

First, we will explain the container formats. Container encapsulate encoded audio, video, subtitles, and metadata of the media content. There are many widely used container format like mp4, avi, mkv. Second, we will discuss encoding formats. Encoding is a process for compressing the audio-video data, and the decoding is opposite. Third, we will discuss raw files. It’s uncompressed audio-video data, contains full information about the video or audio. Based on these discussion, you will get a clear understanding about the fundamental concepts about audio-video processing. This can help you to build the whole knowledge architecture about the media processing.

Based on the understanding of general concepts, in the second part, we will discuss how to utilize these knowledge and pick appropriate element in GStreamer to parses container fiels, decodes, and encodes audio-video data. We will skip the processing raw file in this article because it contains much more content which can not covered in an article.

### Container

Container is a type of box, holding together different types of data, like audio and video streams, subtitles, metadata, and other information. Container use muxing to combine multiple data streams into a single file. During playback, a demuxer seperate the streams, allowing the media player to decode and render the video, audio, and subtitles as intended.

There are many popular container formats. The most commonly used containers for media on the web are MPEG-4 (MP4), Web Media File (WEBM), and MPEG Audio Layer III (MP3). MP4 has great balance between the file size and quality, which makes it to be widely used container format for online distribution. The modern browsers support playing MP4 videos through the HTML5 `<video>` element. Browsers like Chrome, Firefox, Safari, includes built-in decoders for the common video codecs used within MP4 containers.

### Encoding

Encoding is a process of compressing raw video or audio content to a coding format with smaller size. The encoding is typically lossy, meaning that the compressed video or audio lacks some information present in the original file. Although this decreases the quality of the media files, makes them easier to transmissed or stored.

Encoding algorithm includes a lot of details, we can’t explain all of them in a single article, but we can discuss some basic concepts to make sure you have a simple understanding of this. This could give you insperation in the future when you trying to optimize the video streaming quality. The audio encoding algorithm also has a lot of details, but we won’t cover them in this article.

The simplest understanding about the encoding is a processing of compressing video files to reduce their size while maintaining **acceptable** quality.

**Motion estimation and compensation**

A video is made up of many frames, you can image many individual pictures shown quickly on after another. Each frame is a single image displayed at a give frequency, creating the illusion of motion. In a typical video, adjencent frames are often similar, resulting in redundant information that can be compressed.

During video encoding, motion estimation and compensation are used to predict the movement of objects between adjacent frames, reducing redundancy. Frames are then categorized as I-frames, P-frames, or B-frames based on teh encoding strategy, which determines how they depend on each other:

- I-frame. A complete frame encoded independently, containing all the information to display and can be a reference for other frames. It doesn’t rely on motion estimation or compensation.
- P-frame. A frame encoded based on the differences from the previous I or P frame using motion estimation and compensation, it requires the earlier frame to be decoded first.
- B-frame. A frame encoded using motion estimation and compensation to predict differences from both the previous and next I-frame or P-frame, requiring both for decoding.

GOP (group of pictures) is a set of consecutive frames in a video, starting with an I-frame and including P and B frames. The GOP structure organize frames to optimize compression efficiency and enable random access.

**Transform coding**

**Quantization**

Encoding formats can be understand as some standard about how to compress the media files. The compressed media files has smaller size which makes them easier to storage or transmission. The simple work principle of media file encoding is to remove the redundant details from the raw files, and generate most of each frame of video based on the mathematical calculation instead of recording every frame.

There are many encoded formats for audio and video. **H.264** is most common used video compression standard in today. **ACC** is a widely used audio coding standard for compressing the audio raw files. These encoding standard can significantly reducing file sizes while preserving good quality.

### Raw files

Raw files are uncompressed media data generated directly from capturing devices such as cameras, microphones, or other sensors that can record audio or video.

The raw files retains the maximum quality and details of the original signal, which makes them suitable for media processing. For example, you can tweaks raw video stream’s width, height, framerate, or other properties, to makes them usable for downstream.

### Interrelationship

These concepts are related. The raw files captured from recording devices can be encoded to any format. And the encoded audio, video and other necessary informations can be packed in a specific container format.

During playing, the process is completely opposite. The player will load the file and decodes the streams, and play the raw stream. Sometimes the raw stream needs to be processed before rendering them to user.

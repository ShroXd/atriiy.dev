---
title: 'How Rolldown Works: Chunks generation and optimization'
publishedAt: 2025-12-09
draft: true
tags:
  ['rolldown', 'vite', 'rust', 'chunks']
---

## JSON Glossary Entries

These work as before, showing content from the JSON file:

- When working with bundlers, you might encounter a [ModuleIdx](abbr:ModuleIdx) in your build output.
- Code splitting can be configured using [match groups](abbr:match_group) to optimize your bundles.

# Introduction

Code generate stage. TODO.

# Generate chunks

Some basic data structure should be introduced before the real content. But we can fold the details, to makes the reader could search them when it's necessary.

```rust
let mut chunk_graph = ChunkGraph::new(self.link_output.module_table.modules.len());
let mut index_splitting_info: IndexSplittingInfo = // BitSet for each module
let mut bits_to_chunk = FxHashMap::with_capacity(self.link_output.entries.len());
let input_base = ArcStr::from(self.get_common_dir_of_all_modules(...));
```


Based on the `self.option.reverse_module`, use different way to handle it.
    - True, generate chunks for each modules
    - False, use algorithm to split the module. We will focus on this whole algorithm

TODO. explain the reverse module logic here, and use following section for the non-reverse module. Basically it includes `init_entry_point` and `split_chunks` functions.

Two main branching strategies.

Path A. preserve modules mode.
Path B. normal bundling mode.

## Path B

Entry point creation. `init_entry_point()`.

### Split chunks

This function essentially transforms the module dependency graph into an efficient chunk allocation strategy. Also apply automatic splitting logic and user-defined code splitting rules.

The split_chunks function includes several steps:
1. entry analysis. `determine_reachable_modules_for_entry` does BFS on the module graph. Starts from each entry, go through all modules. and assign the entry_index to the modules' bits so that we can split the modules to chunks based on this.
2. manual splitting. according to the user-defined groups, which includes pattern matching and function, to split the modules to chunks first. this is because the user has more business information so that make it better than the auto algo.
3. module assignment. group by bts pattern matching. This is a auto algo to makes the chunks not to large, not too small.
4. if its possible, merge and optimize the chunks.


#### Determine reachable modules for entry

When Rolldown performs code splitting, it needs to figure out which modules each entry point can reach. The `determine_reachable_modules_for_entry` function solves this using a breadth-first graph traversal.

The dependencies graph is built during the scan & link stage, the function fetch the dependencies vector via the [module index](abbr:ModuleIdx) on the `link_output.module_table`. The BFS is processed on the module dependency graph via a `VecDeque`, processing modules level by level from the entry point.

For each modules that was scanned, the `entry_index` will be set on its `bits`. This is used for code splitting algorithm, we will discuss it later. All of these information will be stored in the `index_splitting_info`, which is a `IndexVec`. The type defination is `IndexVec<ModuleIdx, SplittingInfo>`. We can use `module_idx` to refer it easily. And the defination of `SplittingInfo` is:

```rust
pub struct SplittingInfo {
  pub bits: BitSet,
  pub share_count: u32,
}
```

This reachability information later determines which modules go into which chunks during the final bundle generation. We will go through the determining process later.

#### Manual code splitting

1. Early exit checks. This is used for extract the code splitting related configrations. Exit the function if there's no configrations.

2. Module group creation phase. 

```rust
let mut name_to_module_groups: FxHashMap<(usize, ArcStr), ModuleGroupIdx>
let mut index_module_groups: IndexVec<ModuleGroupIdx, ModuleGroup>
```

This is a two level data structure design. We get the `match_group_index` and [match group](abbr:match_group.md) from the `match_groups` stored in the `chunking_options`, we use it to create a related index. And the group related data is stored underthe `ModuleGroup` data structure, which can be refered by the index, stored in the `IndexVec`. Such design avoid the inefficient linear seach and also good for sorting after this step.

`name_to_module_group` is a hash map that maps `(match_group_index, group_name)` to `ModuleGroupIdx`. And the related data could be found in the `index_module_groups`, which is used for storing module groups.

```rust
struct ModuleGroup {
  name: ArcStr,
  match_group_index: usize,
  modules: FxHashSet<ModuleIdx>,
  priority: u32,
  sizes: f64,
}
```



Next, iterate all modules stored in the module table, which is scanned and processed during the scan & link stage. For each module, use each match group to test it. Determining if current module should be put into this group. User can use regex or function in the match group, Rolldown will execute them to check the current module. After that, the module size will be checked if the `allow_min_module_size` and `allow_max_module_size` is setted. The `match_group_index` and `group_name` will be combined as the unique key of module group. At the end of the processing, if the `include_dependencies_recursively` is true, the dependencies will be added to the group recursively. `add_module_and_dependencies_to_group_recursively` is a classic depth-first recursive function on dependencies graph. Finally, all related modules will be added in the group. Pay attention, all dependencies will be added with bypassing the size and shar-count checks.

In the rolldown's design, the group means root plus deps. And the manual group match is only for the _entry_ selection, nto every dependency.


3. Module group sorting and filtering.

Module group will be sorted according to its priority. For the same priority, the one with lower index goes first. For the same priority and index, use dictionary order to sort. Since the sorted vector will be consumed via `pop`, therefore there is a outer `Reverse`.

4. Runtime Module Handling

Manually pull out the runtime module into a standalone chunk and also remove the runtime module from other module groups if it exists.

5. Main processing loop

This is the core of the manual code splitting algorithm. The module group is a stack, we have already sort it according to the priority in the previous step. Now we can use `pop` to iterate the module groups according to the priority. The module groups which is smaller than the allow min size will be skipped. The large module groups will be splitted and make sure its size in the given scope. And this part is the most interested part.

First, preprocessing the modules in this module group according to its size, stable id and executation order. The purpose of doing this is for better segemation. Because there is a giant module which is bigger than the max group size limitation at left part of the module vector, the algorithm won't be able to segementate it.

Second, algorithm iterate the modules from left and right at the same time. Because the given module group is a range, therefore we need to make sure the division won't create a tiny module group that can't match the minimaize size configuration. More specificaly, if `next_right_index + 1 < next_left_index` is true, it means this group can't be split into two groups with both satisfied the min size requirement. Rolldown will ignore the max size requirement and keep the group as whole in this case.

Once the split index is confirmed, the algorithm will try to let the left group includes as many as possible modules until it reach the allowed max size, while ensuring the right module group matches the requirements. Finally, push these two module group into the module group stack. If the right module group is still can be split, it will follow the same logic in the next loop, otherwise, the module group will be skip until next large module group appears.  

After the module groups split loop finished, the chunks will be created based on the result. And the modules in current module group will be removed from other groups.

#### 


  - Step 3: In the main loop, any unassigned module is either placed into an
    existing chunk if its bitset already maps to a chunk (entry/advanced) or
    queued in pending_common_chunks when optimization is allowed. If optimization
    is disallowed, a new common chunk is created immediately.
  - Step 4: try_insert_common_module_to_exist_chunk tries to merge each pending
    common group into an existing entry chunk when safe (respecting strict entry
    signatures), otherwise makes a new common chunk.

#### Automatic chunk assignment

Assign modules to corresponding chunks
Create shared chunks to store modules that belong to multiple chunks

TODO. explain the `pending_common_chunks`

#### Chunk optimization

try insert common module to exist chunk
optimize facade dynamic entry chunks


#### Algo.

Optimized and configurable chunks' size.

## Post processing phase

1. Merge external import namespaces at chunk level
2. Sort modules within chunks
3. Assign execution order to chunks
4. Create final sorted chunk index vector
5. Find entry-level external modules

## Chunk ordering strategy

1. User-defined entry chunks come first (by index)
2. Other chunks ordered by exec order
3. Entry chunks before common chunks when exec order are equal

-----

Summary of Rolldown's Code Splitting Algorithm

**Entry Points and Discovery**: Rolldown starts with user-defined entry points from your config, but automatically discovers additional "virtual" entry points from dynamic imports (`import()` statements) in your code. Each entry point gets assigned a unique bit position (0, 1, 2, etc.).

**BitSet Reachability Analysis**: The algorithm performs a depth-first traversal from each entry point, marking every reachable module with that entry's bit. Each module accumulates a BitSet representing which entry points can reach it - for example, a module reachable from entries 0 and 2 would have BitSet `101`.

**Pattern-Based Chunking**: Modules with identical BitSets (same reachability patterns) are grouped into the same chunk. The algorithm maintains a `bits_to_chunk` HashMap that maps each unique BitSet pattern to a chunk, enabling perfect deduplication.

**Manual Code Splitting Integration**: Before the automatic BitSet-based splitting runs, rolldown applies user-defined manual code splitting rules using pattern matching (regex) or functions. Modules captured by manual rules are marked as assigned and excluded from automatic splitting.

**Optimal Chunk Creation**: The final result is mathematically optimal chunking where shared dependencies automatically become common chunks, entry-specific code stays separate, and modules with identical usage patterns are co-located for optimal caching and loading performance.

**Real-World Benefits**: This approach delivers faster initial loads (users download only what they need), better caching (shared code cached once), and near-instant subsequent navigation - all with minimal configuration since the algorithm discovers optimal boundaries from your actual code structure.
